from __future__ import print_function
import numpy as np
import random
import pytest

from local_perturbations.streaming_hmm.generate import *
from local_perturbations.streaming_hmm.fit import *
from local_perturbations.streaming_hmm.api import train_batch, score_batch

from tests.unit.streaming_hmm.fixtures import (
    fixed_streaming_hmm_model,
    flipping_streaming_hmm_model,
    agnostic_streaming_hmm_model,
)

VERBOSE_TEST = False
if VERBOSE_TEST:
    hmm_fixtures = [
        fixed_streaming_hmm_model(),
        flipping_streaming_hmm_model(),
        agnostic_streaming_hmm_model(),
    ]
else:
    hmm_fixtures = [fixed_streaming_hmm_model()]


@pytest.mark.parametrize("streaming_hmm_model", hmm_fixtures)
def test_train_api(streaming_hmm_model, N_samples=5000):
    """
    Test that the Cappe algorithm (online EM for HMM) produces fitted parameters
    which are sufficiently close to known fixed parameters which generated
    simulated data.
    """
    ### generate HMM sequences (given init, transit, emiss)
    vocab = ["ON", "OFF", "UNSURE"]
    sample = generator(N_samples, vocab, streaming_hmm_model, seed=None)
    print("\n First 100 tokens in sample")
    print(sample[:100])
    print("\n")

    ### fit online HMM algo
    Y = [vocab.index(x) for x in sample]  # rep the observations as numeric
    streaming_hmm_model_fitted, cappe_params_fitted = train_batch(Y, K=2, W=3, t_min=20)

    ### check that fitted parameters are sufficiently close to the true parameters
    is_expected(streaming_hmm_model_fitted, streaming_hmm_model, print_mode=True)


def is_expected(streaming_hmm_model_fitted, streaming_hmm_model, print_mode=True):
    if print_mode:
        print("Transition matrix (Q) fitted :")
        print(streaming_hmm_model_fitted.Q)
        print("Transition matrix (Q) true:")
        print(streaming_hmm_model.Q)
        print("Emissions distributions (gs) fitted :")
        print(streaming_hmm_model_fitted.gs)
        print("Emissions distributions (gs)  true:")
        print(streaming_hmm_model.gs)
    np.testing.assert_allclose(
        streaming_hmm_model_fitted.Q, streaming_hmm_model.Q, atol=0.20
    )
    np.testing.assert_allclose(
        streaming_hmm_model_fitted.gs, streaming_hmm_model.gs, atol=0.20
    )


@pytest.mark.parametrize("streaming_hmm_model", hmm_fixtures)
def test_score_api(
    streaming_hmm_model, mean_score_diff=0.05, N_samples=5000, print_mode=True
):
    """
    Attributes:
        mean_score_diff: Float
            The lower bound on acceptable difference in mean scores
            between data generated by a model and randomly generated data.
            The 'score' here is a predictive probability of the next observation.
        N_samples: Int
            Number of samples used in the scored sequence
    """

    ### generate HMM sequence from provided parameters
    vocab = ["ON", "OFF", "UNSURE"]
    sample = generator(N_samples, vocab, streaming_hmm_model, seed=None)
    print("\n First 100 tokens in sample")
    print(sample[:100])
    print("\n")
    Y = [vocab.index(x) for x in sample]  # rep the observations as numeric

    ### score that sequence with provided parameters
    scores = score_batch(Y, streaming_hmm_model, override_pi=True)
    scores_mean = np.mean(scores)

    ### score a random sequence with provided parameters
    Y_diff = [random.randint(0, streaming_hmm_model.K - 1) for i in range(N_samples)]
    scores_diff = score_batch(Y_diff, streaming_hmm_model, override_pi=True)
    scores_diff_mean = np.mean(scores_diff)

    ### check scores should be higher when score on train data then from random
    if print_mode:
        print("Mean score on self distribution %.04f" % (scores_mean))
        print("Mean score on random distribution %.04f" % (scores_diff_mean))
    assert scores_mean - scores_diff_mean > mean_score_diff
